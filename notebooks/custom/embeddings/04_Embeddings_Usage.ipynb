{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Cell 1 - Imports and Setup\n",
    "from pathlib import Path\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from txtai.embeddings import Embeddings\n",
    "import logging\n",
    "import json\n",
    "\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s'\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Embeddings Usage Examples\n",
    "\n",
    "This notebook demonstrates all standard search and management operations with txtai embeddings:\n",
    "1. Document Operations (add, delete, update)\n",
    "2. Search Types (semantic, SQL, hybrid)\n",
    "3. Query Patterns\n",
    "4. Result Processing\n",
    "\n",
    "## Cells 2-3: Configuration\n",
    "The previous notebooks (01_Embeddings_Overview, 02_Content_Storage, 03_Cloud_Storage) explain the configuration options for the embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-17 15:07:51,283 - INFO - Initializing embeddings...\n"
     ]
    }
   ],
   "source": [
    "# Cell 2 - Standard Configuration\n",
    "config = {\n",
    "    \"path\": \"sentence-transformers/nli-mpnet-base-v2\",\n",
    "    \"content\": True,\n",
    "    \"backend\": \"faiss\",\n",
    "    \"hybrid\": True,\n",
    "    \"scoring\": {\n",
    "        \"method\": \"bm25\",\n",
    "        \"terms\": True,\n",
    "        \"normalize\": True\n",
    "    },\n",
    "    \"normalize\": True\n",
    "}\n",
    "\n",
    "logger.info(\"Initializing embeddings...\")\n",
    "embeddings = Embeddings(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-17 15:07:52,035 - INFO - Indexing test documents...\n"
     ]
    }
   ],
   "source": [
    "# Cell 3 - Test Documents\n",
    "test_docs = [\n",
    "    {\n",
    "        \"id\": \"doc1\",\n",
    "        \"text\": \"Machine learning models require significant computational resources\",\n",
    "        \"metadata\": {\n",
    "            \"category\": \"tech\",\n",
    "            \"tags\": [\"ML\", \"computing\"],\n",
    "            \"priority\": 1\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        \"id\": \"doc2\",\n",
    "        \"text\": \"Cloud computing enables scalable infrastructure solutions\",\n",
    "        \"metadata\": {\n",
    "            \"category\": \"tech\",\n",
    "            \"tags\": [\"cloud\", \"infrastructure\"],\n",
    "            \"priority\": 2\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        \"id\": \"doc3\",\n",
    "        \"text\": \"Natural language processing transforms text into structured data\",\n",
    "        \"metadata\": {\n",
    "            \"category\": \"tech\",\n",
    "            \"tags\": [\"NLP\", \"data\"],\n",
    "            \"priority\": 1\n",
    "        }\n",
    "    }\n",
    "]\n",
    "\n",
    "# Index documents\n",
    "logger.info(\"Indexing test documents...\")\n",
    "embeddings.index([(doc[\"id\"], doc[\"text\"], json.dumps(doc[\"metadata\"])) for doc in test_docs])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Cells 4-6: Standard Search Operations\n",
    "\n",
    "The embeddings index combines semantic search with SQL to provide a flexible search interface.\n",
    "\n",
    "We can search using semantic similarity, SQL, or a hybrid of the two.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 4 - SQL Search\n",
    "\n",
    "SQL search is like a SQL database.\n",
    "\n",
    "We are simply filtering the results based on the SQL query.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-17 15:07:52,205 - INFO - \n",
      "Testing SQL Search Operations:\n",
      "2024-11-17 15:07:52,206 - INFO - \n",
      "1. Basic SQL Filter:\n",
      "2024-11-17 15:07:52,206 - INFO - No results found\n",
      "2024-11-17 15:07:52,207 - INFO - \n",
      "2. Complex SQL Filter:\n"
     ]
    }
   ],
   "source": [
    "# Cell 4 - SQL Search\n",
    "def test_sql_search():\n",
    "    logger.info(\"\\nTesting SQL Search Operations:\")\n",
    "\n",
    "    # Basic SQL filter\n",
    "    logger.info(\"\\n1. Basic SQL Filter:\")\n",
    "    basic_query = \"\"\"\n",
    "        select text, score\n",
    "        from txtai\n",
    "        where metadata like '%technical%'\n",
    "    \"\"\"\n",
    "    sql_results = embeddings.search(\n",
    "        basic_query,\n",
    "        limit=1\n",
    "    )\n",
    "\n",
    "    # Process results safely\n",
    "    if sql_results:\n",
    "        logger.info(f\"SQL-filtered Result: {sql_results[0]['text']}\")\n",
    "        logger.info(f\"Score: {sql_results[0]['score']}\")\n",
    "    else:\n",
    "        logger.info(\"No results found\")\n",
    "\n",
    "    # Complex SQL filter\n",
    "    logger.info(\"\\n2. Complex SQL Filter:\")\n",
    "    complex_query = \"\"\"\n",
    "        select text, score\n",
    "        from txtai\n",
    "        where metadata like '%tech%'\n",
    "        and score > 0.5\n",
    "        order by score desc\n",
    "    \"\"\"\n",
    "    complex_results = embeddings.search(\n",
    "        complex_query,\n",
    "        limit=2\n",
    "    )\n",
    "\n",
    "    # Process complex results\n",
    "    for idx, result in enumerate(complex_results, 1):\n",
    "        logger.info(f\"\\nResult {idx}:\")\n",
    "        logger.info(f\"Text: {result['text']}\")\n",
    "        logger.info(f\"Score: {result['score']}\")\n",
    "\n",
    "test_sql_search()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 5 - Basic Semantic Search\n",
    "\n",
    "The basic semantic search option is like RAG without the LLM.\n",
    "\n",
    "We are just measuring the similarity between the query and the indexed documents, and returning the most similar documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-17 15:07:52,221 - INFO - \n",
      "Testing different search operations:\n",
      "2024-11-17 15:07:52,222 - INFO - \n",
      "Basic Semantic Search:\n",
      "2024-11-17 15:07:52,250 - INFO - Semantic Search Result: Machine learning models require significant computational resources\n",
      "2024-11-17 15:07:52,250 - INFO - Score: 0.6655701879559184\n"
     ]
    }
   ],
   "source": [
    "# Cell 5 - Basic Search Operations\n",
    "def test_semantic_search():\n",
    "    logger.info(\"\\nTesting different search operations:\")\n",
    "\n",
    "    ## Basic Semantic Search\n",
    "    logger.info(\"\\nBasic Semantic Search:\")\n",
    "    semantic_results = embeddings.search(\"computational resources for AI\", 1)\n",
    "    logger.info(f\"Semantic Search Result: {semantic_results[0]['text']}\")\n",
    "    logger.info(f\"Score: {semantic_results[0]['score']}\")\n",
    "\n",
    "test_semantic_search()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 6 - Hybrid Search\n",
    "\n",
    "Hybrid search combines semantic search with SQL.\n",
    "\n",
    "It uses the benefits of lakehouse architecture to combine the speed of semantic search with the flexibility of SQL.\n",
    "\n",
    "Basically, the lakehouse architecture (txtai embeddings) has a single centralized method of managing the embeddings.\n",
    "\n",
    "This means we can start the query by narrowing the search space using SQL, and then use semantic search to find the most similar documents, all in one query operation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-17 15:07:52,255 - INFO - \n",
      "Testing different search operations:\n",
      "2024-11-17 15:07:52,255 - INFO - \n",
      "2. Hybrid Search:\n",
      "2024-11-17 15:07:52,269 - INFO - Hybrid Search Result: Machine learning models require significant computational resources\n",
      "2024-11-17 15:07:52,271 - INFO - Score: 0.6688058558998728\n"
     ]
    }
   ],
   "source": [
    "# Cell 6 - Basic Search Operations\n",
    "def test_hybrid_search():\n",
    "    logger.info(\"\\nTesting different search operations:\")\n",
    "    # 2. Hybrid Search (using config with hybrid=True)\n",
    "    logger.info(\"\\n2. Hybrid Search:\")\n",
    "    hybrid_results = embeddings.search(\n",
    "        \"machine learning infrastructure\",\n",
    "        1\n",
    "    )\n",
    "    logger.info(f\"Hybrid Search Result: {hybrid_results[0]['text']}\")\n",
    "    logger.info(f\"Score: {hybrid_results[0]['score']}\")\n",
    "\n",
    "test_hybrid_search()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cells 7-9: Document Management Operations\n",
    "\n",
    "The document management operations are used to add, update, and delete documents from the index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-17 15:07:52,276 - INFO - \n",
      "Testing Add Document Operations:\n",
      "2024-11-17 15:07:52,297 - INFO - New document search result: Data visualization helps understand complex patterns\n",
      "2024-11-17 15:07:52,297 - INFO - Score: 0.6688700570231481\n"
     ]
    }
   ],
   "source": [
    "# Cell 7 - Add Document Operations\n",
    "def test_add_document():\n",
    "    logger.info(\"\\nTesting Add Document Operations:\")\n",
    "\n",
    "    new_doc = {\n",
    "        \"id\": \"doc4\",\n",
    "        \"text\": \"Data visualization helps understand complex patterns\",\n",
    "        \"metadata\": {\n",
    "            \"category\": \"tech\",\n",
    "            \"tags\": [\"visualization\", \"data\"],\n",
    "            \"priority\": 2\n",
    "        }\n",
    "    }\n",
    "\n",
    "    # Single document addition\n",
    "    embeddings.index([(new_doc[\"id\"], new_doc[\"text\"], json.dumps(new_doc[\"metadata\"]))])\n",
    "\n",
    "    # Verify addition\n",
    "    results = embeddings.search(\"visualization patterns\", 1)\n",
    "    logger.info(f\"New document search result: {results[0]['text']}\")\n",
    "    logger.info(f\"Score: {results[0]['score']}\")\n",
    "\n",
    "test_add_document()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cell 8 - Document Update Operations\n",
    "\n",
    "Now we'll test updating an existing document in the index. This includes:\n",
    "1. Updating the document text\n",
    "2. Preserving the metadata\n",
    "3. Verifying the update through search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-17 15:07:52,304 - INFO - \n",
      "Testing Update Document Operations:\n",
      "2024-11-17 15:07:52,318 - INFO - Before update - Text: Data visualization helps understand complex patterns\n",
      "2024-11-17 15:07:52,321 - INFO - Before update - Score: 0.5180126488468576\n",
      "2024-11-17 15:07:52,343 - INFO - After update - Text: Advanced data visualization reveals hidden patterns\n",
      "2024-11-17 15:07:52,343 - INFO - After update - Score: 0.6751291705733342\n"
     ]
    }
   ],
   "source": [
    "# Cell 8 - Update Document Operations\n",
    "def test_update_document():\n",
    "    logger.info(\"\\nTesting Update Document Operations:\")\n",
    "\n",
    "    # Initial search to show current document\n",
    "    initial_results = embeddings.search(\"visualization\", 1)\n",
    "    logger.info(f\"Before update - Text: {initial_results[0]['text']}\")\n",
    "    logger.info(f\"Before update - Score: {initial_results[0]['score']}\")\n",
    "\n",
    "    # Update by re-indexing the document\n",
    "    doc_id = \"doc4\"\n",
    "    updated_text = \"Advanced data visualization reveals hidden patterns\"\n",
    "    updated_metadata = {\n",
    "        \"category\": \"tech\",\n",
    "        \"tags\": [\"visualization\", \"data\", \"advanced\"],\n",
    "        \"priority\": 1\n",
    "    }\n",
    "\n",
    "    # Re-index to update\n",
    "    embeddings.index([(doc_id, updated_text, json.dumps(updated_metadata))])\n",
    "\n",
    "    # Verify update\n",
    "    results = embeddings.search(\"advanced visualization\", 1)\n",
    "    logger.info(f\"After update - Text: {results[0]['text']}\")\n",
    "    logger.info(f\"After update - Score: {results[0]['score']}\")\n",
    "\n",
    "test_update_document()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Document Delete Operations\n",
    "\n",
    "Finally, we'll test document deletion:\n",
    "1. Delete a specific document\n",
    "2. Verify the deletion through search\n",
    "3. Confirm the document is no longer retrievable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-17 15:07:52,351 - INFO - \n",
      "Testing Delete Document Operations:\n",
      "2024-11-17 15:07:52,370 - INFO - Before deletion - Text: Advanced data visualization reveals hidden patterns\n",
      "2024-11-17 15:07:52,370 - INFO - Before deletion - Score: 0.47253335120891815\n",
      "2024-11-17 15:07:52,371 - INFO - Deleted document with ID: doc4\n",
      "2024-11-17 15:07:52,390 - INFO - No matching results found after deletion\n"
     ]
    }
   ],
   "source": [
    "# Cell 9 - Delete Document Operations\n",
    "def test_delete_document():\n",
    "    logger.info(\"\\nTesting Delete Document Operations:\")\n",
    "\n",
    "    # Initial search to confirm document exists\n",
    "    initial_results = embeddings.search(\"visualization\", 1)\n",
    "    logger.info(f\"Before deletion - Text: {initial_results[0]['text']}\")\n",
    "    logger.info(f\"Before deletion - Score: {initial_results[0]['score']}\")\n",
    "\n",
    "    # Delete document\n",
    "    doc_id = \"doc4\"\n",
    "    embeddings.delete([doc_id])\n",
    "    logger.info(f\"Deleted document with ID: {doc_id}\")\n",
    "\n",
    "    # Verify deletion by searching for the same content\n",
    "    post_results = embeddings.search(\"visualization\", 1)\n",
    "    if post_results and post_results[0]['score'] < initial_results[0]['score']:\n",
    "        logger.info(\"Document successfully deleted\")\n",
    "        logger.info(f\"New top result - Text: {post_results[0]['text']}\")\n",
    "        logger.info(f\"New top result - Score: {post_results[0]['score']}\")\n",
    "    else:\n",
    "        logger.info(\"No matching results found after deletion\")\n",
    "\n",
    "test_delete_document()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "txtai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
